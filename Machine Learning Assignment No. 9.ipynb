{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that  represent the underlying problem better for the predictive models, resulting in improved model accuracy on unseen data. The features in your data will directly influence the predictive models you use and the results you can achieve. Domain knowledge of data is key to the process. Along with domain knowledge, both programming and math skills are required to perform feature engineering. The correct features make machine learning algorithms successful.\n",
    "\n",
    "**Features influence the results of predictive models:**\n",
    "\n",
    "1. Reduced complexity\n",
    "2. Increased accuracy\n",
    "\n",
    "**Three general types of features:**\n",
    "\n",
    "1. Categorical features\n",
    "2. Text features\n",
    "3. Image features\n",
    "\n",
    "**5 Feature Engineering Techniques:**\n",
    "\n",
    "1. Imputation\n",
    "2. Outliers\n",
    "3. Log Transform\n",
    "4. Binning\n",
    "5. Feature Spli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "**Feature selection is the process of reducing the number of input variables when developing a predictive model.**\n",
    "\n",
    "**Aim of feature selection is to decrease learning time, performance, and increase model interpretability.**\n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the **choice of statistical measures depends on the data type of both the input and output variables.**\n",
    "\n",
    "As such, it can be challenging for a machine learning practitioner to select an appropriate statistical measure for a dataset when performing filter-based feature selection.\n",
    "\n",
    "Feature selection methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable. Feature selection is primarily focused on removing non-informative or redundant predictors from the model.\n",
    "\n",
    "Some predictive modeling problems have a large number of variables that can slow the development and training of models and require a large amount of system memory. Additionally, the performance of some models can degrade when including input variables that are not relevant to the target variable.\n",
    "\n",
    "**Feature selection as follows:**\n",
    "\n",
    "- Feature Selection: Select a subset of input features from the dataset.\n",
    "\n",
    "    - Unsupervised: Do not use the target variable (e.g. remove redundant variables).\n",
    "\n",
    "        - Correlation\n",
    "\n",
    "    - Supervised: Use the target variable (e.g. remove irrelevant variables).\n",
    "\n",
    "        - Wrapper: Search for well-performing subsets of features.\n",
    "\n",
    "            - RFE\n",
    "\n",
    "        - Filter: Select subsets of features based on their relationship with the target.\n",
    "\n",
    "            - Statistical Methods\n",
    "\n",
    "            - Feature Importance Methods\n",
    "\n",
    "        - Intrinsic: Algorithms that perform automatic feature selection during training.\n",
    "\n",
    "             - Decision Trees\n",
    "\n",
    "- Dimensionality Reduction: Project input data into a lower-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?**\n",
    "\n",
    "**Solution:-**\n",
    "Filter methods are generally used as a pre-processing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficient.\n",
    "\n",
    "Feature------- Continuous-------------- Categorical \n",
    "Continuous---- Pearson's Correlation--- LDA\n",
    "Categoriacal-- Anova------------------- Chi-Square\n",
    "\n",
    "- Pearson’s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1.\n",
    "\n",
    "- LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "\n",
    "- ANOVA: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n",
    "\n",
    "- Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n",
    "\n",
    "**Filter Merits:**\n",
    "- Computationally cheaper as compared to wrapper and embedded. \n",
    "- Fastest running time. \n",
    "- Lower risk of over fitting. \n",
    "- Ability of good generalization. \n",
    "- Easily scale to high-dimensional datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- No interaction with classification model for feature selection. \n",
    "- Mostly ignores feature dependencies and considers each feature separately in case of univariate techniques, which may lead to low computational performance as compared to other techniques of feature selection.\n",
    "\n",
    "**In wrapper methods** try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "- Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "- Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "- Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "**Wrapper Merits:**\n",
    "- Interacts with the classifier for feature selection. \n",
    "- More comprehensive search of feature-set space. \n",
    "- Considers feature dependencies. \n",
    "- Better generalization than filter approach\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- High computational cost. \n",
    "- Longer running time. \n",
    "- Higher risk of over fitting as compared to filter and embedded. \n",
    "- More computationally unfeasible with increased number of features No guarantee of optimality of the solution if predicted with another classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. i. Describe the overall feature selection process.**\n",
    "\n",
    "**Feature selection is the process of reducing the number of input variables when developing a predictive model.**\n",
    "\n",
    "**Aim of feature selection is to decrease learning time, performance, and increase model interpretability.**\n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the **choice of statistical measures depends on the data type of both the input and output variables.**\n",
    "\n",
    "As such, it can be challenging for a machine learning practitioner to select an appropriate statistical measure for a dataset when performing filter-based feature selection.\n",
    "\n",
    "Feature selection methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable. Feature selection is primarily focused on removing non-informative or redundant predictors from the model.\n",
    "\n",
    "Some predictive modeling problems have a large number of variables that can slow the development and training of models and require a large amount of system memory. Additionally, the performance of some models can degrade when including input variables that are not relevant to the target variable.\n",
    "\n",
    "**Feature selection as follows:**\n",
    "\n",
    "- Feature Selection: Select a subset of input features from the dataset.\n",
    "\n",
    "    - Unsupervised: Do not use the target variable (e.g. remove redundant variables).\n",
    "\n",
    "        - Correlation\n",
    "\n",
    "    - Supervised: Use the target variable (e.g. remove irrelevant variables).\n",
    "\n",
    "        - Wrapper: Search for well-performing subsets of features.\n",
    "\n",
    "            - RFE\n",
    "\n",
    "        - Filter: Select subsets of features based on their relationship with the target.\n",
    "\n",
    "            - Statistical Methods\n",
    "\n",
    "            - Feature Importance Methods\n",
    "\n",
    "        - Intrinsic: Algorithms that perform automatic feature selection during training.\n",
    "\n",
    "             - Decision Trees\n",
    "\n",
    "- Dimensionality Reduction: Project input data into a lower-dimensional feature space.\n",
    "\n",
    "ii. **Feature Extraction:** By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables.\n",
    "\n",
    "i. PCA (Principal Component Analysis)\n",
    "\n",
    "**ii. LDA (Linear Discriminant Analysis)**\n",
    "\n",
    "The feature Extraction technique gives us new features which are a linear combination of the existing features. The new set of features will have different values as compared to the original feature values. The main aim is that fewer features will be required to capture the same information. We might think that choosing fewer features might lead to underfitting but in the case of the Feature Extraction technique, the extra data is generally noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Describe the feature engineering process in the sense of a text categorization issue.**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data. Text classification is the task of automatically placing pre-defined labels on previously unseen documents. Used in document indexing, e-mail filtering, web browsing, and personal information agents, text classification is an active and important area of research where machine learning and information retrieval (IR) intersect.\n",
    "\n",
    "**Steps:**\n",
    "1. Environment setup: import packages and read data.\n",
    "2. Language detection: understand which natural language data is in.\n",
    "3. Text pre-processing: text cleaning and transformation.\n",
    "4. Length analysis: measured with different metrics.\n",
    "5. Sentiment analysis: determine whether a text is positive or negative.\n",
    "6. Named-Entity recognition: tag text with pre-defined categories such as person names, organizations, locations.\n",
    "7. Word frequency: find the most important n-grams.\n",
    "8. Word vectors: transform a word into numbers.\n",
    "9. Topic modeling: extract the main topics from corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.**\n",
    "\n",
    "**Solution:-** \n",
    "\n",
    "Cosine similarity **measures the similarity between two vectors of an inner product space**. It is **measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction**. It is often used to measure document similarity in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "numerator = (2*2)+(3*1)+(2*0)+(0*0)+(2*3)+(3*2)+(3*1)+(0*3)+(1*1)\n",
    "\n",
    "denominator = sqrt(4+9+4+4+9+9+1)*sqrt(4+1+9+4+1+9+1)\n",
    "\n",
    "similarity = numerator/denominator\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:-** Hamming distance between two vectors is the number of bits we must change to change one into the other. d=min {d(x,y):x,y∈C,x≠y}. \n",
    "\n",
    "The Hamming distance of two given lines of code is the number of points at which the lines' binary code values are different (assuming that the two lines of code are the same length).\n",
    "\n",
    "Hamming gap between 10001011 and 11001111 is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).**\n",
    "\n",
    "Simple matching coefficient counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:-** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jacc_ind1 = (2/4)*100\n",
    "\n",
    "SMC1 = 5/8\n",
    "\n",
    "print(Jacc_ind1)\n",
    "\n",
    "print(SMC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "B = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jacc_ind2 = (2/4)*100\n",
    "\n",
    "SMC2 = 3/8\n",
    "\n",
    "print(Jacc_ind2)\n",
    "\n",
    "print(SMC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:-**\n",
    "\n",
    "High Dimensional means that the number of dimensions are staggeringly high — so high that calculations become extremely difficult. With high dimensional data, the number of features can exceed the number of observations\n",
    "\n",
    "Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse.\n",
    "\n",
    "High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data.\n",
    "\n",
    "Seven Techniques for Data Dimensionality Reduction\n",
    "\n",
    "1. Missing Values Ratio.\n",
    "2. Low Variance Filter.\n",
    "3. High Correlation Filter.\n",
    "4. Random Forests / Ensemble Trees.\n",
    "5. Principal Component Analysis (PCA).\n",
    "6. Backward Feature Elimination.\n",
    "7. Forward Feature Construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Make a few quick notes on:**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "**i. PCA is an acronym for Personal Computer Analysis.**\n",
    "\n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "**ii. Use of vectors**\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction and are drawn as arrows.\n",
    "\n",
    "**iii. Embedded technique**\n",
    "\n",
    "Embedded methods complete the feature selection process within the construction of the machine learning algorithm itself. In other words, they perform feature selection during the model training, which is why we call them embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Make a comparison between:**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "**i. Sequential backward exclusion vs. sequential forward selection**\n",
    "\n",
    "sequential forward selection:--- Against a defined metric, first feature is chosen after a model is trained and produces the metric. Next feature is selected by searching for that feature which when clubbed with the first chosen feature gives best performance metric results via model training. More and more features are selected one by one while measuring the contribution of the clubbed features in each iteration against the metric using model trained on clubbed features till the required number of features are selected.\n",
    "\n",
    "Sequential backward exclusion:--- A model is trained on all features and the metric score is calculated. In each iteration, a feature is found which if removed from the collection of features, improves the metric score. This goes on and features are discarded one by one, improving performance metric score with each iteration until predefined number of features are left.\n",
    "\n",
    "**ii. Function selection methods: filter vs. wrapper**\n",
    "\n",
    "Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Wrapper approach is an iterative approach in feature selection where all combinations of subset of features is used to train the model and a perfomance metric is used to test the model and check of there is an improvement of score over the previous iteration where a different subset of features were used. Examples of this technique are forward feature selection and backward feature selection.\n",
    "\n",
    "**iii. SMC vs. Jaccard coefficient**\n",
    "\n",
    "SMC or Simple Matching Index is defined as the number of matching attributes divided by total number of attributes. SMC can only be applied to observations with equal number of data points. SMC is most useful in binary attributes.\n",
    "\n",
    "Jaccard coefficient, also called Jaccard index is a measure of similarity of sets. It is defined as Modulus of A intersection B divided by modulus of A union B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
